{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = True,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str\n        Path to the serialized LLM.\n\n    clear_mem : bool\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n                \n            #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n\n            quantization_config = transformers.BitsAndBytesConfig(\n                load_in_4bit = True,\n                bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n                bnb_4bit_use_double_quant = False,\n                bnb_4bit_compute_dtype=torch.float16,\n            )\n            \n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n        #if not load_in_8bit:\n        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], batch_size: 32\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        verbose : bool, default=False\n            Display progress bar.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n\n        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n        for j in range(batches):\n            \n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = input_texts[a:b]\n        \n            with torch.no_grad():\n\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                    padding=True\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                label = model_inputs['input_ids']\n                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = label[..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                loss = loss.view(len(logits), -1)\n                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n                loss = torch.sum(loss, -1) / valid_length\n\n                loss_list += loss.cpu().tolist()\n\n                # Debug output\n                #print(f\"\\nProcessing: '{text}'\")\n                #print(f\"With special tokens: '{text_with_special}'\")\n                #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                #print(f\"Individual losses: {loss.tolist()}\")\n                #print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        # print(\"\\nFinal perplexities:\")\n        # for text, perp in zip(input_texts, ppl):\n        #     print(f\"Text: '{text}'\")\n        #     print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()\n\n# LOAD GEMMA SCORER\nscorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')\n\n\nimport re, sys\n\nclass Reprinter:\n    def __init__(self):\n        self.text = ''\n\n    def moveup(self, lines):\n        for _ in range(lines):\n            sys.stdout.write(\"\\x1b[A\")\n\n    def reprint(self, text):\n        # Clear previous text by overwritig non-spaces with spaces\n        self.moveup(self.text.count(\"\\n\"))\n        sys.stdout.write(re.sub(r\"[^\\s]\", \" \", self.text))\n\n        # Print new text\n        lines = min(self.text.count(\"\\n\"), text.count(\"\\n\"))\n        self.moveup(lines)\n        sys.stdout.write(text)\n        self.text = text\n\nreprinter = Reprinter()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T23:45:05.662691Z","iopub.execute_input":"2025-01-10T23:45:05.663116Z","iopub.status.idle":"2025-01-10T23:47:54.246920Z","shell.execute_reply.started":"2025-01-10T23:45:05.663082Z","shell.execute_reply":"2025-01-10T23:47:54.246166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"words_0 = [\n    'reindeer', 'mistletoe', 'elf', 'gingerbread', 'family', 'advent', 'scrooge', 'chimney', \n    'fireplace', 'ornament'\n]\nwords_1 = [\n    'reindeer', 'sleep', 'walk', 'the', 'night', 'and', 'drive', 'mistletoe', 'scrooge', 'laugh', \n    'chimney', 'jump', 'elf', 'bake', 'gingerbread', 'family', 'give', 'advent', 'fireplace', \n    'ornament'\n]\nwords_2 = [\n    'sleigh', 'yuletide', 'beard', 'carol', 'cheer', 'chimney', 'decorations', 'gifts', 'grinch', \n    'holiday', 'holly', 'jingle', 'magi', 'naughty', 'nice', 'nutcracker', 'ornament', 'polar', \n    'workshop', 'stocking'\n]\nwords_3 = [\n    'sleigh', 'of', 'the', 'magi', 'yuletide', 'cheer', 'is', 'unwrap', 'gifts', 'and', 'eat', \n    'cheer', 'holiday', 'decorations', 'holly', 'jingle', 'relax', 'sing', 'carol', 'visit', \n    'workshop', 'grinch', 'naughty', 'nice', 'chimney', 'stocking', 'ornament', 'nutcracker', \n    'polar', 'beard'\n]\nwords_4 = [\n    'from', 'and', 'of', 'to', 'the', 'as', 'in', 'that', 'it', 'we', 'with', 'not', 'you', \n    'have', 'milk', 'chocolate', 'candy', 'peppermint', 'eggnog', 'cookie', 'fruitcake', 'toy', \n    'doll', 'game', 'puzzle', 'greeting', 'card', 'wrapping', 'paper', 'bow', 'wreath', 'poinsettia', \n    'snowglobe', 'candle', 'fireplace', 'wish', 'dream', 'hope', 'believe', 'wonder', 'night', \n    'star', 'angel', 'peace', 'joy', 'season', 'merry', 'hohoho', 'kaggle', 'workshop'\n]\nwords_5 = [\n    'from', 'and', 'and', 'as', 'we', 'and', 'have', 'the', 'in', 'is', 'it', 'of', 'not', \n    'that', 'the', 'to', 'with', 'you', 'advent', 'card', 'angel', 'bake', 'beard', 'believe', \n    'bow', 'candy', 'candle', 'carol', 'cheer', 'cheer', 'chocolate', 'chimney', 'cookie', \n    'decorations', 'doll', 'dream', 'drive', 'eat', 'eggnog', 'family', 'fireplace', 'fireplace', \n    'chimney', 'fruitcake', 'game', 'gifts', 'give', 'gingerbread', 'greeting', 'grinch', 'holiday', \n    'holly', 'hohoho', 'hope', 'jingle', 'jump', 'joy', 'kaggle', 'laugh', 'magi', 'merry', 'milk', \n    'mistletoe', 'naughty', 'nice', 'night', 'night', 'elf', 'nutcracker', 'ornament', 'ornament', \n    'of', 'the', 'wrapping', 'paper', 'peace', 'peppermint', 'polar', 'poinsettia', 'puzzle', \n    'reindeer', 'relax', 'scrooge', 'season', 'sing', 'sleigh', 'sleep', 'snowglobe', 'star', 'stocking', \n    'toy', 'unwrap', 'visit', 'walk', 'wish', 'wonder', 'workshop', 'workshop', 'wreath', 'yuletide'\n]\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T00:43:32.947878Z","iopub.execute_input":"2025-01-11T00:43:32.948227Z","iopub.status.idle":"2025-01-11T00:43:32.956953Z","shell.execute_reply.started":"2025-01-11T00:43:32.948202Z","shell.execute_reply":"2025-01-11T00:43:32.955928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport random\nimport pandas as pd\nimport time\n\n# Assuming `score` function and PerplexityCalculator (scorer) are already available\niteration = 0\n\nstart_time = time.time()\nimport math\n\ndef simulated_annealing(solution, solution_df, row_id_column_name, max_iterations=1000, initial_temp=1000, cooling_rate=0.99):\n    \"\"\"\n    Apply Simulated Annealing to improve the given solution.\n    \n    Args:\n    - solution: The current solution (list of words).\n    - solution_df: The DataFrame used to calculate fitness.\n    - row_id_column_name: The name of the column in the DataFrame to calculate perplexity.\n    - max_iterations: Maximum iterations for the SA process.\n    - initial_temp: Initial temperature.\n    - cooling_rate: The rate at which the temperature decreases.\n    \n    Returns:\n    - The improved solution and its fitness score.\n    \"\"\"\n    current_solution = solution[:]\n    current_fitness = calculate_fitness([current_solution], solution_df, row_id_column_name)[0]  # Get fitness (lower is better)\n    \n    best_solution = current_solution[:]\n    best_fitness = current_fitness\n\n    # Initial temperature\n    temperature = initial_temp\n\n    # Perform iterations\n    for iteration in range(max_iterations):\n        # Create a neighboring solution by swapping two random words\n        neighbor = current_solution[:]\n        i, j = random.sample(range(len(neighbor)), 2)  # Pick two random indices\n        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]  # Swap them\n        \n        # Calculate the fitness of the new solution\n        neighbor_fitness = calculate_fitness([neighbor], solution_df, row_id_column_name)[0]\n\n        # If the new solution is better, accept it\n        if neighbor_fitness < current_fitness:\n            current_solution = neighbor\n            current_fitness = neighbor_fitness\n        else:\n            # Otherwise, accept it with a certain probability based on temperature\n            acceptance_probability = math.exp((current_fitness - neighbor_fitness) / temperature)\n            if random.random() < acceptance_probability:\n                current_solution = neighbor\n                current_fitness = neighbor_fitness\n\n        # Update the best solution if the new solution is better\n        if current_fitness < best_fitness:\n            best_solution = current_solution[:]\n            best_fitness = current_fitness\n        \n        # Reduce the temperature\n        temperature *= cooling_rate\n\n    return best_solution, best_fitness\n\n# Genetic Algorithm Functions\ndef create_population(size, words):\n    \"\"\" Create an initial population of random permutations of the given words. \"\"\"\n    population = []\n    for _ in range(size):\n        random.shuffle(words)\n        population.append(words[:])  # make a copy of the shuffled list\n    return population\n\ndef calculate_fitness(population, solution_df, row_id_column_name):\n    \"\"\" Calculate the fitness of each individual (lower perplexity is better). \"\"\"\n    fitness_scores = []\n    for individual in population:\n        # Create a string for the permuted text\n        permuted_text = ' '.join(individual)\n\n        # Calculate perplexity using the scorer (PerplexityCalculator)\n        perplexity = scorer.get_perplexity([permuted_text], 4)[0]  # Assuming scorer.get_perplexity returns a list\n        fitness_scores.append(perplexity)  # lower perplexity is better\n        \n        # Optionally print iteration and perplexity (using reprinter)\n        global iteration\n        iteration += 1\n        reprinter.reprint(f\"Iteration: {iteration}, Perplexity: {perplexity:.2f}\\r\")\n\n    return fitness_scores\n\ndef tournament_selection(population, fitness_scores, num_parents, tournament_size=3):\n    selected_parents = []\n    for _ in range(num_parents):\n        # Tournament selection: pick a random sample of individuals and select the best one\n        tournament = random.sample(list(zip(population, fitness_scores)), tournament_size)\n        tournament.sort(key=lambda x: x[1])  # Sort by fitness (lower perplexity is better)\n        selected_parents.append(tournament[0][0])  # Select the best\n    return selected_parents\n    \ndef roulette_wheel_selection(population, fitness_scores, num_parents=2):\n    \"\"\" Perform roulette wheel selection to select parents. \"\"\"\n    total_fitness = sum(fitness_scores)\n    selection_probs = [1 - (score / total_fitness) for score in fitness_scores]  # Lower perplexity -> higher probability\n    selected_parents = random.choices(population, weights=selection_probs, k=num_parents)\n    return selected_parents\n\n\n\ndef elitism(population, fitness_scores, num_elites=2):\n    \"\"\" Select the best individuals (elites) to survive. \"\"\"\n    sorted_population = sorted(zip(population, fitness_scores), key=lambda x: x[1])\n    elites = [indiv[0] for indiv in sorted_population[:num_elites]]\n    return elites\n\n\ndef pmx_crossover(parent1, parent2):\n    \"\"\" Perform Partially Matched Crossover (PMX) between two parents. \"\"\"\n    size = len(parent1)\n    point1, point2 = sorted(random.sample(range(size), 2))  # Select two random crossover points\n\n    # Create offspring with the same structure as parents\n    offspring1 = [None] * size\n    offspring2 = [None] * size\n\n    # Copy the segments from parents\n    for i in range(point1, point2):\n        offspring1[i] = parent2[i]\n        offspring2[i] = parent1[i]\n\n    # Mapping to resolve duplicates\n    mapping1 = {parent2[i]: parent1[i] for i in range(point1, point2)}\n    mapping2 = {parent1[i]: parent2[i] for i in range(point1, point2)}\n\n    # Fill the remaining positions using the mapping\n    for i in range(size):\n        if offspring1[i] is None:\n            word = parent1[i]\n            while word in offspring1:  # Handle duplicates in offspring1\n                word = mapping1[word]\n            offspring1[i] = word\n\n        if offspring2[i] is None:\n            word = parent2[i]\n            while word in offspring2:  # Handle duplicates in offspring2\n                word = mapping2[word]\n            offspring2[i] = word\n\n    return offspring1, offspring2\n\n\ndef simulated_annealing_schedule(initial_temp, cooling_rate, iteration, max_iterations):\n    \"\"\"Calculate the current temperature and mutation probability based on the annealing schedule.\"\"\"\n    temperature = initial_temp * (cooling_rate ** (iteration / max_iterations))\n    return temperature\n\ndef accept_worse_solution(perplexity, best_perplexity, temperature):\n    \"\"\"Accept a worse solution based on the simulated annealing probability.\"\"\"\n    delta = perplexity - best_perplexity\n    if delta < 0:\n        return True  # Always accept a better solution\n    else:\n        probability = math.exp(-delta / temperature)\n        return random.random() < probability\n\ndef mutate(individual, mutation_rate, temperature, best_perplexity, current_perplexity):\n    \"\"\" Apply mutation with simulated annealing acceptance criterion. \"\"\"\n    if random.random() < mutation_rate:\n        # Perform mutation (swap two random elements)\n        i, j = random.sample(range(len(individual)), 2)\n        individual[i], individual[j] = individual[j], individual[i]\n        \n        # Check if we should accept this mutation based on temperature\n        if not accept_worse_solution(current_perplexity, best_perplexity, temperature):\n            # Revert mutation if it's not accepted\n            individual[i], individual[j] = individual[j], individual[i]\n\n    return individual\n\n\ndef genetic_algorithm(words, solution_df, row_id_column_name, generations=100, population_size=20, mutation_rate=0.1, num_parents=2, log_filename='generation_log.json', fixed_parents=None, num_elites=3, sa_iterations=1000, initial_temp=1000, cooling_rate=0.99, sa_frequency=5, stagnation_threshold=5, mutation_increase_factor=1.5 ):\n    population = create_population(population_size, words)\n    best_solution = None\n    best_fitness = float('inf')\n    logs = []  # To store logs for each iteration\n    stagnation_count = 0  # Counter to track stagnation\n\n    for generation in range(generations):\n        print(f\"Generation {generation + 1}/{generations}\")\n\n        # Calculate fitness scores for the current population\n        fitness_scores = calculate_fitness(population, solution_df, row_id_column_name)\n\n        # Find the best solution in the current population\n        best_generation_fitness = min(fitness_scores)\n        best_generation_solution = population[fitness_scores.index(best_generation_fitness)]\n\n        # Update the global best solution if necessary\n        if best_generation_fitness < best_fitness:\n            best_fitness = best_generation_fitness\n            best_solution = best_generation_solution\n            stagnation_count = 0  # Reset stagnation counter when there is improvement\n        else:\n            stagnation_count += 1  # Increment stagnation counter if no improvement\n\n        # If stagnation is detected, reinitialize the population or increase mutation rate\n        if stagnation_count >= stagnation_threshold:\n            print(f\"Stagnation detected! Resetting population or adjusting mutation rate.\")\n            # Reset the population or increase mutation rate (you can decide what action to take)\n            mutation_rate *= mutation_increase_factor  # Increase mutation rate\n            population = create_population(population_size, words)  # Reinitialize population\n            fitness_scores = calculate_fitness(population, solution_df, row_id_column_name)  # Recalculate fitness scores\n            stagnation_count = 0  # Reset stagnation count\n            print(f\"New mutation rate: {mutation_rate}\")\n\n        # Log the generation data (text and perplexity)\n        for i, individual in enumerate(population):\n            permuted_text = ' '.join(individual)\n            logs.append({\n                'generation': generation + 1,\n                'individual_index': i,\n                'permuted_text': permuted_text,\n                'perplexity': fitness_scores[i]  # Ensure we are using the updated fitness_scores\n            })\n\n        # Select parents\n        parents = []\n        if fixed_parents:\n            # Use the manually set first two parents, convert them to word lists if necessary\n            parent1 = fixed_parents[0].split()\n            parent2 = fixed_parents[1].split()\n            parents.extend([parent1, parent2])\n\n            # Fill the remaining parents using tournament selection\n            while len(parents) < num_parents:\n                selected_parents = roulette_wheel_selection(population, fitness_scores, num_parents=num_parents - len(parents))\n                parents.extend(selected_parents)\n        else:\n            # If no fixed parents are set, use tournament selection for all parents\n            parents = roulette_wheel_selection(population, fitness_scores, num_parents=num_parents - len(parents))\n\n        # Create the next generation using crossover and mutation\n        next_generation = []\n        # New mutation handling:\n        for i in range(0, len(parents), 2):  # Ensure we are pairing parents properly\n            offspring1, offspring2 = pmx_crossover(parents[i], parents[i+1])  # Apply PMX crossover\n        \n            # Set current_perplexity based on best fitness (perplexity)\n            current_perplexity = best_fitness  # This is the best fitness we've found so far\n            \n            # Calculate the temperature using the current generation\n            temperature = simulated_annealing_schedule(initial_temp, cooling_rate, generation, generations)\n            \n            # Apply mutation with additional parameters\n            offspring1 = mutate(offspring1, mutation_rate, temperature, best_fitness, current_perplexity)  # Mutate the offspring\n            offspring2 = mutate(offspring2, mutation_rate, temperature, best_fitness, current_perplexity)  # Mutate the offspring\n        \n            next_generation.append(offspring1)  # Add mutated offspring to the next generation\n            next_generation.append(offspring2)  # Add mutated offspring to the next generation\n\n\n        # Optionally apply elitism to keep the best individuals\n        elites = elitism(population, fitness_scores, num_elites=num_elites)\n        next_generation.extend(elites)\n\n        population = next_generation\n\n        # Apply Simulated Annealing every `sa_frequency` generations or if a significant improvement is found\n        if generation % sa_frequency == 0:  # Apply SA every `sa_frequency` generations\n            current_solution = ' '.join(best_solution)\n            current_perplexity = best_fitness\n        \n            # Optionally, you could check if there's been enough improvement to trigger SA\n            if current_perplexity > best_fitness:\n                best_solution, best_fitness = simulated_annealing(best_solution, solution_df, row_id_column_name, max_iterations=sa_iterations, initial_temp=initial_temp, cooling_rate=cooling_rate)\n        \n\n    # Save the logs to a JSON file\n    with open(log_filename, 'w') as f:\n        json.dump(logs, f, indent=4)\n\n    # Return the best solution found\n    return ' '.join(best_solution), best_fitness\n\n\n# Words list\nwords = words_1  # Assuming 'words_1' is your list of words\n# Create a dummy solution DataFrame (replace with actual solution dataframe)\nsolution_df = pd.DataFrame({\n    'id': [0],\n    'text': \"words\"\n})\n\n# Apply the genetic algorithm, passing the fixed_parents\nbest_solution, best_fitness = genetic_algorithm(\n    words, solution_df, row_id_column_name=\"id\", generations=10, population_size=30, mutation_rate=0.3,\n    log_filename=\"/kaggle/working/genetic_algorithm_log.json\", fixed_parents=fixed_parents, num_elites=3,\n    sa_iterations=200, initial_temp=100, cooling_rate=0.95, sa_frequency=5, stagnation_threshold=5, mutation_increase_factor=1.5\n)\n\nprint()\nprint(f\"Best permutation: {best_solution}\")\nprint(f\"Best perplexity: {best_fitness}\")\nprint(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:58:03.559396Z","iopub.execute_input":"2025-01-11T02:58:03.559736Z","iopub.status.idle":"2025-01-11T02:58:15.956589Z","shell.execute_reply.started":"2025-01-11T02:58:03.559712Z","shell.execute_reply":"2025-01-11T02:58:15.955669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#words_1\nfixed_parents = ['reindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament', 'ornament mistletoe fireplace chimney and advent elf the family gingerbread reindeer scrooge walk give jump drive bake night sleep laugh']  # Parents are the first two individuals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:42:52.027052Z","iopub.execute_input":"2025-01-11T02:42:52.027384Z","iopub.status.idle":"2025-01-11T02:42:52.031103Z","shell.execute_reply.started":"2025-01-11T02:42:52.027353Z","shell.execute_reply":"2025-01-11T02:42:52.030231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#words_4\nfixed_parents= ['from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow wreath poinsettia snowglobe candle fireplace wish dream hope believe wonder night star angel peace joy season merry hohoho kaggle workshop', 'from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow wreath poinsettia snowglobe candle fireplace wish dream hope believe wonder night star angel peace joy season merry hohoho kaggle workshop']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T01:58:51.297079Z","iopub.execute_input":"2025-01-11T01:58:51.297396Z","iopub.status.idle":"2025-01-11T01:58:51.301200Z","shell.execute_reply.started":"2025-01-11T01:58:51.297370Z","shell.execute_reply":"2025-01-11T01:58:51.300345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fixed_parents=None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T02:47:46.531390Z","iopub.execute_input":"2025-01-11T02:47:46.531670Z","iopub.status.idle":"2025-01-11T02:47:46.535113Z","shell.execute_reply.started":"2025-01-11T02:47:46.531647Z","shell.execute_reply":"2025-01-11T02:47:46.534201Z"}},"outputs":[],"execution_count":null}]}